# Introduction

Over this past year i've committed myself to the dense reading of "Neural Networks and Deep Learning" by Michael Nielsen. This project encmopasses the many hours i've spent diving deep into the math and low-level logic of neural networks.

Initially, this deep nueral network was set to an arbitrary 30 epochs. The results of 30 cycles on the training data are displayed in Figure 1.

![](img/epoch29_output.png)Figure 1

<!-- ctrl + shift + v for markdown preview -->

For a visuallization  of this, we can see the cost on the training data in Figure 2. It slopes down dramitcally between epochs 0 throgh 10. After that, the amount of cost reduction versus time and computational power severly decreases. Interestingly, it seems the graph takes on a y=1/x shape.
![](img/cost_on_training_data.png) Figure 2



